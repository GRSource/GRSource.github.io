(window.webpackJsonp=window.webpackJsonp||[]).push([[72],{614:function(e,a,r){"use strict";r.r(a);var t=r(65),v=Object(t.a)({},(function(){var e=this,a=e.$createElement,r=e._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h1",{attrs:{id:"视频播放器设计-基于ffmpeg"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#视频播放器设计-基于ffmpeg"}},[e._v("#")]),e._v(" 视频播放器设计（基于FFmpeg）")]),e._v(" "),r("h2",{attrs:{id:"videoplayercontroller"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#videoplayercontroller"}},[e._v("#")]),e._v(" VideoPlayerController")]),e._v(" "),r("p",[e._v("调度器，内部维护音视频同步模块、音频 输出模块、视频输出模块，为客户端代码提供开始播放、暂停、继续播 放、停止播放接口;为音频输出模块和视频输出模块提供两个获取数据 的接口。")]),e._v(" "),r("h3",{attrs:{id:"audiooutput"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#audiooutput"}},[e._v("#")]),e._v(" AudioOutput")]),e._v(" "),r("p",[e._v("音频输出模块，由于在不同平台上有不同的实现， 所以这里真正的声音渲染API为Void类型，但是音频的渲染要放在一个 单独的线程(不论是平台API自动提供的线程，还是我们主动建立的线 程)中进行，所以这里有一个线程的变量，在运行过程中会调用注册过 来的回调函数来获取音频数据。")]),e._v(" "),r("h3",{attrs:{id:"avsynchronizer"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#avsynchronizer"}},[e._v("#")]),e._v(" AVSynchronizer")]),e._v(" "),r("p",[e._v("音视频同步模块，音视频同步模块，会组合后文将要讲到的输入模 块、音频队列和视频队列，其主要为它的客户端代码 VideoPlayerController调度器提供接口，包括:开始、结束，以及最重要 的获取音频数据和获取对应时间戳的视频帧。此外，它还会维护一个解 码线程，并且根据音视频队列里面的元素数目来继续或者暂停该解码线 程的运行。")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("VideoDecoder")]),e._v(" "),r("p",[e._v("输入模块，其职责在前面已经分析过了，由于还没 有确定具体的技术实现，所以这里先根据前面的分析暂时写了三个实例 变量，一个是协议层解析器，一个是格式解封装器，一个是解码器，并\n且它主要向AVSynchronizer提供接口:打开文件资源(网络或者本 地)、关闭文件资源、解码出一定时间长度的音视频帧。")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("AudioFrameQueue")]),e._v(" "),r("p",[e._v("音频队列，主要用于存储音频帧，为它的客户 端代码音视频同步模块提供压入和弹出操作，由于解码线程和声音播放 线程会作为生产者和消费者同时访问该队列中的元素，所以该队列要保 证线程安全性。")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("AudioFrame")]),e._v(" "),r("p",[e._v("音频帧，其中记录了音频的数据格式以及这一帧的 具体数据、时间戳等信息。")])])])]),e._v(" "),r("li",[r("p",[e._v("VideoFrameQueue")]),e._v(" "),r("p",[e._v("视频队列，主要用于存储视频帧，为它的客户 端代码音视频同步模块提供压入和弹出操作，由于解码线程和视频播放 线程会作为生产者和消费者同时访问该队列中的元素，所以该队列要保 证线程安全性。")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("VideoFrame")]),e._v(" "),r("p",[e._v("视频帧，记录了视频的格式以及这一帧的具体的数 据、宽、高以及时间戳等信息。")])])])])])])]),e._v(" "),r("h3",{attrs:{id:"videooutput"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#videooutput"}},[e._v("#")]),e._v(" VideoOutput")]),e._v(" "),r("p",[e._v("视频输出模块，虽然这里统一使用OpenGL ES来渲 染视频，但是前文已提到过，OpenGL ES的具体实现在不同的平台上也 会有自己的上下文环境，所以这里采用了Void类型的实现，当然，必须 由我们主动开启一个线程来作为OpenGL ES的渲染线程，它会在运行过 程中调用注册过来的回调函数来获取视频数据。")]),e._v(" "),r("h2",{attrs:{id:"videodecoder解码模块的实现"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#videodecoder解码模块的实现"}},[e._v("#")]),e._v(" VideoDecoder解码模块的实现")]),e._v(" "),r("p",[e._v("单独线程")]),e._v(" "),r("h3",{attrs:{id:"openfile"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#openfile"}},[e._v("#")]),e._v(" openFile")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("得到音视频流的信息")]),e._v(" "),r("p",[e._v("在查看流信息时，实际会发生解码行为，所以解码的数据越多，花费的时间越长，但是得到的流信息越准确，一般通过设置probesize和max_analyze_duration这两个参数来给出探测数据量 的大小和最大的解析数据的长度，其值常设置为50×1024和75000，如果 达到了设置的值却还没有解析出对应的视频流和音频流的MetaData，那么就返回失败，紧接着会进入重试策略")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("音频解码器")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("解码之后libswresample重采样")]),e._v(" "),r("ul",[r("li",[e._v("AudioFrame")])])])])]),e._v(" "),r("li",[r("p",[e._v("视频解码器")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("解码之后通过libswscale将nv12转换为YUV420P")]),e._v(" "),r("ul",[r("li",[e._v("VideoFrame")])])])])])])])]),e._v(" "),r("p",[r("em",[e._v("XMind - Trial Version")])])])}),[],!1,null,null,null);a.default=v.exports}}]);