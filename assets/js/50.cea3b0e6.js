(window.webpackJsonp=window.webpackJsonp||[]).push([[50],{593:function(e,t,r){"use strict";r.r(t);var a=r(65),i=Object(a.a)({},(function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h1",{attrs:{id:"gpuimage"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gpuimage"}},[e._v("#")]),e._v(" GPUImage")]),e._v(" "),r("h2",{attrs:{id:"sources目录"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#sources目录"}},[e._v("#")]),e._v(" Sources目录")]),e._v(" "),r("h3",{attrs:{id:"gpuimageoutput"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gpuimageoutput"}},[e._v("#")]),e._v(" GPUImageOutput")]),e._v(" "),r("p",[e._v("包含outputFramebuffer缓冲对象\n包含addTarget方法")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("GPUImagePicture")]),e._v(" "),r("p",[e._v("初始化Picture，调用initWithImage：参数为image。\n创建纹理单元，绑定纹理。\n1.1.初始化filter，即创建着色器、创建渲染程序、设置uniform值等\n2. 调用addTarget：参数为filter。\n将创建的纹理单元保存到filter，供filter使用，为接下来的filter渲染做准备。\n同时将filter保存在GPUImageOutput的targets数组当中。\n3.filter调用useNextFrameForImageCapture：方法启动信号量用来捕捉下一帧。\n4.picture调用processImage：开始处理图片。\n处理过程是：循环调用target的协议方法来处理不同的filter，因为它遵守GPUImageInput协议，所以用到四个协议方法setCurrentlyReceivingMonochromeInput、setInputSize、setInputFramebuffer、这个newFrameReadyAtTime方法用来激活渲染程序，里面实际调用renderToTextureWithVertices方法，创建帧缓冲，将接下来的纹理操作在新创建的缓冲区进行，渲染。如果有后续的filter添加到当前filter，则会递归执行下一个filter\n这四个方法均在filter中实现。\n5.调用filter中的方法，调用CVOpenGLESTextureCacheCreateTextureFromImage方法根据纹理单元的名称从帧缓冲当中取出缓冲图片。")])]),e._v(" "),r("li",[r("p",[e._v("GPUImageVideoCamera")]),e._v(" "),r("p",[e._v("1.调用initWithSessionPreset初始化AVCaptureSession、AVCaptureDeviceInput、AVCaptureVideoDataOutput，并设置videoOutput代理、创建用于yuv转换成rgb的着色器和渲染程序\n2.创建filter(filter包含着色器的创建)，然后调用addTarget方法，将filter添加至targets数组当中，此时还未开始采集，所以我们暂时无需绑定纹理。\n3.创建GPUImageView用于显示渲染结果，添加到当前view的子视图，调用filter的addTarget方法将其添加到filter当中\n4.开始采集，在videoOutput代理方法中处理采样得到的数据，即通过CVOpenGLESTextureCacheCreateTextureFromImage函数提取y分量和uv分量，通过CVOpenGLESTextureGetName方法获取y和uv分量对应的纹理单元，分别将他们进行绑定以便我们后续可以配置纹理。\n4.将yuv渲染成rgb操作。convertYUVToRGBOutput，存储到帧缓冲当中。\n5.循环执行所有targets，调用target的协议方法来处理不同的filter，因为它遵守GPUImageInput协议，所以用到四个协议方法setCurrentlyReceivingMonochromeInput、setInputSize、setInputFramebuffer、这个newFrameReadyAtTime方法用来激活渲染程序，里面实际调用renderToTextureWithVertices方法，创建帧缓冲，将接下来的纹理操作在新创建的缓冲区进行，渲染。如果当前filter有addTarget其他的filter，则递归执行下一个filter（将前一个filter的输出帧缓冲传递给下一个filter的输入帧缓冲。实际上不一定是filter，可以是所有遵守GPUImageInput协议的类，例如GPUImageView）。\n这四个方法均在filter中实现。\n6.在GPUImageView当中创建着色器、渲染帧缓冲到layer上。")])])]),e._v(" "),r("h2",{attrs:{id:"filters目录"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#filters目录"}},[e._v("#")]),e._v(" Filters目录")]),e._v(" "),r("h3",{attrs:{id:"gpuimagefilter"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gpuimagefilter"}},[e._v("#")]),e._v(" GPUImageFilter")]),e._v(" "),r("p",[e._v("渲染输出的结果操作都是在filter当中进行的。")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("GPUImageColorMatrixFilter")]),e._v(" "),r("ul",[r("li",[e._v("GPUImageSepiaFilter")])])]),e._v(" "),r("li",[r("p",[e._v("GPUImageGammaFilter")])])]),e._v(" "),r("h2",{attrs:{id:"gpuimageinput协议"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gpuimageinput协议"}},[e._v("#")]),e._v(" GPUImageInput协议")]),e._v(" "),r("p",[r("em",[e._v("XMind - Trial Version")])])])}),[],!1,null,null,null);t.default=i.exports}}]);